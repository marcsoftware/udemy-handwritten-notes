1
00:00:01,650 --> 00:00:08,500
Hello cloud gurus and welcome to this lecture and this lecture is going to be a summary of S3.

2
00:00:08,610 --> 00:00:10,710
So it's the end of this section.

3
00:00:10,710 --> 00:00:13,550
Congratulations on making it so far.

4
00:00:13,740 --> 00:00:19,770
This is just gonna be all summary of everything that we've learned about S3 including all of our exam

5
00:00:19,770 --> 00:00:20,350
tips.

6
00:00:20,370 --> 00:00:21,540
So let's get started.

7
00:00:22,910 --> 00:00:25,340
Remember that S3 is object based.

8
00:00:25,340 --> 00:00:34,220
So it allows you to upload your files images documents files can be from 0 bytes to 5 terabytes in size.

9
00:00:34,970 --> 00:00:37,340
And there is unlimited storage.

10
00:00:37,430 --> 00:00:41,410
You don't have to worry about capacity management or capacity plumbing.

11
00:00:41,510 --> 00:00:42,970
They do all of that for you.

12
00:00:43,010 --> 00:00:46,240
And Amazon just add storage arrays as and when they need it.

13
00:00:46,250 --> 00:00:51,710
So you don't need to worry about capacity planning and the files are stored in buckets and you should

14
00:00:51,710 --> 00:00:53,710
know by now when you see bucket.

15
00:00:53,720 --> 00:01:00,440
Just think folder is three is a universal namespace with globally unique names.

16
00:01:00,560 --> 00:01:06,550
And here's our example below showing HECS then is S3 and the region name.

17
00:01:06,680 --> 00:01:10,830
Then after Amazon AWOL Stockholm it shows us the bucket name.

18
00:01:10,840 --> 00:01:17,180
So in this example a cloud guru for the consistency model we have read after right.

19
00:01:17,210 --> 00:01:21,160
Consistency for puts of brand new objects into S3.

20
00:01:21,170 --> 00:01:27,800
However when we overwrite that file or we change it or we delete it and you try to access the same file

21
00:01:27,800 --> 00:01:33,530
immediately you may still see the older version for a while or the UN deleted version.

22
00:01:33,530 --> 00:01:36,590
So any changes or deletions are not immediate.

23
00:01:36,590 --> 00:01:42,060
Instead they use a model called eventual consistency for overwrite put and delete.

24
00:01:42,080 --> 00:01:49,100
We have multiple different storage tiers or storage classes for S3 beginning with the regular S3.

25
00:01:49,100 --> 00:01:55,280
So that is durable immediately available and suitable for frequently accessed files.

26
00:01:55,280 --> 00:02:03,740
Then we have S3 II which again is durable and immediately available however it's more suitable for infrequently

27
00:02:03,740 --> 00:02:04,800
access files.

28
00:02:04,820 --> 00:02:09,770
We then have S3 ones own ie a and this is the same as I.

29
00:02:09,830 --> 00:02:17,420
However data is stored only in a single availability zone and this means the one zone ie a service is

30
00:02:17,420 --> 00:02:18,800
not as resilient.

31
00:02:18,800 --> 00:02:23,940
It doesn't have the same level of availability as you get for the regular S3.

32
00:02:24,050 --> 00:02:30,050
So it's good for data where a lower level of availability is acceptable and you do pay 20 percent less

33
00:02:30,050 --> 00:02:32,550
than the regular S3 for this service.

34
00:02:32,570 --> 00:02:35,710
We then have the S3 reduce redundancy storage.

35
00:02:35,900 --> 00:02:39,620
And remember this is for data that is easily reproducible.

36
00:02:39,620 --> 00:02:45,850
For example thumbnails where you own the original image and you're storing it in a different location.

37
00:02:46,010 --> 00:02:51,500
And if you were to lose the thumbnail data stored in reduced redundancy storage you would easily be

38
00:02:51,500 --> 00:02:53,670
able to reproduce those files.

39
00:02:53,900 --> 00:02:59,060
Now reduce redundancy storage used to be more cost effective than the regular S3.

40
00:02:59,060 --> 00:03:05,150
However that has now changed and Amazon are now recommending that for most workloads we use the regular

41
00:03:05,150 --> 00:03:06,090
S3.

42
00:03:06,110 --> 00:03:09,520
However we do think it's worth knowing for the exam.

43
00:03:09,560 --> 00:03:17,630
Finally we have Glacier which is used for archiving and is most suitable for situations where it's acceptable

44
00:03:17,990 --> 00:03:22,080
to wait between 3 and 5 hours before accessing your storage.

45
00:03:22,190 --> 00:03:29,300
So glacier is most appropriate to use for historical files archive data files that you don't need to

46
00:03:29,300 --> 00:03:35,240
access very frequently and when you do access them you can wait between three to five hours.

47
00:03:35,240 --> 00:03:41,700
Remember the core fundamentals of an S3 object so you have the key which is the name of the file.

48
00:03:41,780 --> 00:03:49,850
The value which is the data contained within the file version I.D. metadata which is data about data

49
00:03:50,090 --> 00:03:55,580
and then we have sub resources which are used to manage the bucket specific configuration.

50
00:03:55,580 --> 00:04:01,760
For example bucket policies and access control lists which are used to control both of those are used

51
00:04:01,760 --> 00:04:07,400
to control access to your buckets and the contents of your bucket cross origin resource sharing.

52
00:04:07,400 --> 00:04:13,790
So remember that's when we enable the contents of one bucket to see the contents of another bucket or

53
00:04:13,790 --> 00:04:18,880
more widely when we allow one resource to access another resource within AWB.

54
00:04:19,160 --> 00:04:26,780
And there's also transfer acceleration and transfer acceleration simply speeds up the uploads of objects

55
00:04:26,870 --> 00:04:32,180
into S3 utilizing the cloud front network of edge locations.

56
00:04:32,180 --> 00:04:38,750
Remember the S3 is for object storage only so it's only for files not suitable for operating systems

57
00:04:38,840 --> 00:04:47,810
not suitable for databases and successful uploads will generate the ATP 200 status code in terms of

58
00:04:47,810 --> 00:04:51,740
security all newly created buckets are private.

59
00:04:51,740 --> 00:04:56,960
So by default there is no public access to your buckets when you initially create them.

60
00:04:57,170 --> 00:05:03,920
You can set up access control in your buckets using bucket policies and these are applied at a bucket

61
00:05:03,920 --> 00:05:04,480
level.

62
00:05:04,880 --> 00:05:08,620
So that means they apply to the bucket and the contents of the bucket.

63
00:05:08,630 --> 00:05:16,340
You also have access control lists and these are applied on an object level so you can create an access

64
00:05:16,340 --> 00:05:22,610
control list which only applies to an individual object and you can have different access control lists

65
00:05:22,890 --> 00:05:25,460
applied to different objects within the same bucket.

66
00:05:26,440 --> 00:05:32,050
And if you want to keep a record of all the different requests made to the S3 bucket you can do that

67
00:05:32,050 --> 00:05:37,750
using access logs and these can be configured on the bucket and either written to the same bucket or

68
00:05:37,750 --> 00:05:42,910
written into another bucket and these will capture all of the access requests made into your bucket

69
00:05:43,180 --> 00:05:46,610
as three supports a number of different types of encryption.

70
00:05:46,870 --> 00:05:55,060
So we have encryption in transit and that is via SSL or TALF and this is when any request made into

71
00:05:55,060 --> 00:06:02,430
S3 is encrypted in transit or encrypted on the network and we can see that when we make a request into

72
00:06:02,430 --> 00:06:08,550
a pocket using HECS and there is also encryption at rest.

73
00:06:08,550 --> 00:06:14,760
And for encryption at rest we have either server side encryption or client side encryption and for server

74
00:06:14,760 --> 00:06:15,630
side encryption.

75
00:06:15,630 --> 00:06:17,660
There are three different options.

76
00:06:17,700 --> 00:06:26,460
We have the SSA S3 and this is where each object is encrypted with a unique key using strong multi factor

77
00:06:26,460 --> 00:06:33,610
encryption and the keys are managed within S3 and a W S manage this end to end.

78
00:06:33,690 --> 00:06:40,710
So they do all of the encryption for you and decryption and SSA S3 is also sometimes called advanced

79
00:06:40,710 --> 00:06:51,840
encryption standard 256 bit SSA KLM s is similar to SSA S3 but with some added benefits so you get the

80
00:06:51,840 --> 00:06:59,580
separate permissions for an envelope key and this envelope key protects your data's encryption key giving

81
00:06:59,580 --> 00:07:05,740
you added protection against unauthorized access to your objects within S3.

82
00:07:06,060 --> 00:07:12,960
And this also gives you an audit trail telling you when keys were used and by whom and if you're playing

83
00:07:12,960 --> 00:07:19,200
around with SC S3 SC came mess depending on what you're doing you might see some charges within your

84
00:07:19,200 --> 00:07:19,640
account.

85
00:07:20,010 --> 00:07:25,290
So just be aware of that and make sure that once you finish setting something up and once you understand

86
00:07:25,290 --> 00:07:31,430
how it works just remember to go in and delete all those resources SNCC.

87
00:07:31,440 --> 00:07:39,660
So this is the customer provided key where you manage your own key and a W S just take care of the encryption

88
00:07:39,990 --> 00:07:46,590
and decryption process so they decrypt it for you when you want to read your files but ultimately the

89
00:07:46,590 --> 00:07:53,460
key belongs to you and you have to manage the key so you have to manage its lifecycle when it expires.

90
00:07:53,460 --> 00:07:56,670
You have to manage rotating the key yourself.

91
00:07:56,670 --> 00:08:02,170
So just remember those three different types of server side encryption and remember the differences

92
00:08:02,190 --> 00:08:07,350
between them and then you should be able to answer any question that they throw at you in the exam.

93
00:08:07,350 --> 00:08:10,030
We also have client side encryption.

94
00:08:10,440 --> 00:08:16,900
So this is where you encrypt the file on your own platform before you actually upload it into S3.

95
00:08:17,130 --> 00:08:22,980
So you can encrypt it by any mechanism that you choose and the file is encrypted locally an S3 just

96
00:08:22,980 --> 00:08:25,470
treats it as an encrypted disk.

97
00:08:25,480 --> 00:08:32,160
And finally remember that we can use a pocket policy to prevent any unencrypted files from being uploaded

98
00:08:32,160 --> 00:08:33,510
into S3.

99
00:08:33,540 --> 00:08:42,600
So in the lab we created our policy which only allowed a request which included X AMC server side encryption

100
00:08:42,600 --> 00:08:45,330
parameter within the request header.

101
00:08:45,330 --> 00:08:50,730
And then when we tried to upload a file which did not include the correct parameter within that header

102
00:08:51,450 --> 00:08:53,650
we were prevented from uploading the object.

103
00:08:53,670 --> 00:08:55,310
So we saw that it failed.

104
00:08:55,440 --> 00:09:03,390
Cross origin resource sharing or cause is used to enable cross origin access for your AWOL resources

105
00:09:03,510 --> 00:09:11,280
for example an S3 hosted Web site accessing JavaScript or image files which are located in a completely

106
00:09:11,280 --> 00:09:17,430
different S3 bucket and you should know that by default the resources in one bucket cannot access the

107
00:09:17,430 --> 00:09:19,080
resources located in another.

108
00:09:19,170 --> 00:09:24,660
And if we want to set this up we need to configure cross origin resource sharing on the bucket being

109
00:09:24,720 --> 00:09:30,990
accessed and enable access for the origin or bucket attempting to access.

110
00:09:30,990 --> 00:09:36,930
So remember we did this in the lab with our two buckets containing the index or HDL and the load page

111
00:09:36,930 --> 00:09:37,790
to HCM.

112
00:09:37,890 --> 00:09:43,710
And we set up access on that bucket containing load page to HDMI out to allow access for the origin

113
00:09:43,770 --> 00:09:46,770
within which indexed or HMO was located.

114
00:09:46,770 --> 00:09:53,120
And when we do that remember that we always need to use the S3 Web site are and not the regular bucket.

115
00:09:53,130 --> 00:09:56,850
You are well and just to remind you we've got the two examples here.

116
00:09:56,850 --> 00:09:59,110
So the first one highlighted in orange.

117
00:09:59,130 --> 00:10:01,500
That is our S3 Web site U.R.L..

118
00:10:01,530 --> 00:10:06,240
So it goes HDP then a cloud guru which is our bucket name.

119
00:10:06,240 --> 00:10:09,290
And then the big clue here is three dash Web site.

120
00:10:09,330 --> 00:10:15,750
And then our region a US one Amazon NWS dot com and then our regular bucket you are well which is the

121
00:10:15,750 --> 00:10:19,880
HDP s and then our region name Amazon NWS dot com.

122
00:10:19,950 --> 00:10:21,570
And finally the bucket name.

123
00:10:21,570 --> 00:10:26,670
So just remember when you see that you are out and it says S3 dash website and then your region name

124
00:10:27,060 --> 00:10:31,450
that is our S3 website you are all moving on to cloud front.

125
00:10:31,500 --> 00:10:35,640
Remember the terminology for the different elements that make up cloud front.

126
00:10:35,790 --> 00:10:38,130
So we have the edge location.

127
00:10:38,130 --> 00:10:41,180
This is the location where the content is going to be caged.

128
00:10:41,280 --> 00:10:45,840
And this is separate to an AWOL region or a W S AC.

129
00:10:45,840 --> 00:10:51,480
We also have the origin and this is the location of all the files that the content delivery network

130
00:10:51,510 --> 00:10:53,180
is going to distribute.

131
00:10:53,190 --> 00:10:55,830
And remember this may be an S3 bucket.

132
00:10:55,830 --> 00:11:02,950
It can also be an easy to instance or an elastic balancer or Route 53 distribution.

133
00:11:02,950 --> 00:11:09,160
This is the name given to the content delivery network and this consists of a collection of edge locations

134
00:11:09,340 --> 00:11:15,210
and of course when we did the lab and we created our content delivery network we created a distribution.

135
00:11:15,300 --> 00:11:22,180
And when we tried to access our files we did so using the distribution domain name and we got two different

136
00:11:22,180 --> 00:11:23,610
types of distribution.

137
00:11:23,650 --> 00:11:28,840
We have the Web distribution and that's what we use for Web sites and that's the type of distribution

138
00:11:29,170 --> 00:11:30,610
that we set up in the lab.

139
00:11:30,610 --> 00:11:39,820
We also have Aarti MP which is Adobe's real time messaging protocol and this is used for flash or media

140
00:11:39,820 --> 00:11:40,570
streaming.

141
00:11:40,630 --> 00:11:46,150
And remember the edge locations are not just read only you can also write to them.

142
00:11:46,150 --> 00:11:53,980
So you can also upload objects into the location and Amazon will handle the transfer of the object from

143
00:11:53,980 --> 00:12:00,670
the location into your S3 Buckets and objects are kept in the cage for the time to live or the TTL and

144
00:12:00,670 --> 00:12:07,390
the default time to live is 24 hours after that 24 hours the time to live expires and the objects are

145
00:12:07,390 --> 00:12:09,040
then cleared from the cage.

146
00:12:09,100 --> 00:12:14,530
And if your files are changing more frequently than every 24 hours you can clear those from the cage

147
00:12:14,560 --> 00:12:15,500
manually.

148
00:12:15,490 --> 00:12:17,470
It's called an invalidation.

149
00:12:17,470 --> 00:12:24,190
And remember in the lab we could configure in validations on that in validations tab on an object basis

150
00:12:24,250 --> 00:12:26,840
but you will be charged if you want to do this.

151
00:12:26,890 --> 00:12:34,240
There are two main approaches for performance optimization within S3 for the get intensive workloads.

152
00:12:34,240 --> 00:12:40,390
You simply use cloud from for example if you have many users logging onto your website or accessing

153
00:12:40,390 --> 00:12:42,610
files located in your S3 bucket.

154
00:12:42,610 --> 00:12:46,120
The easiest way to optimise that is to use cloud front.

155
00:12:46,120 --> 00:12:47,800
However for mixed workloads.

156
00:12:47,800 --> 00:12:55,540
So when you have a mix of get put list bucket get bucket the best way to optimise performance is to

157
00:12:55,540 --> 00:13:02,680
avoid using sequential key names for your objects and instead use a random prefix like a hex hash at

158
00:13:02,680 --> 00:13:04,210
the beginning of the key name.

159
00:13:04,210 --> 00:13:10,330
And this just stops as three from storing all of your objects on the same physical partition.

160
00:13:10,660 --> 00:13:14,530
So just see the example below for these object names.

161
00:13:14,530 --> 00:13:18,840
We've added the random hex hash as a prefix to the object name.

162
00:13:19,120 --> 00:13:25,420
And this will force S3 to store them on different physical partitions and just avoid the possibility

163
00:13:25,420 --> 00:13:31,300
of IO contention that you can sometimes get when you have many files stored on the same partition and

164
00:13:31,300 --> 00:13:34,150
many people trying to access at the same time.

165
00:13:34,450 --> 00:13:41,350
And our final tip for the exam is to make sure you read the S3 AFAIK you as S3 was one of the first

166
00:13:41,350 --> 00:13:43,680
services that was bought out by Amazon.

167
00:13:43,720 --> 00:13:46,900
That can often be a lot of questions relating to S3.

168
00:13:46,960 --> 00:13:49,780
So this is a really good one to know inside out.

169
00:13:49,780 --> 00:13:53,330
So definitely make sure you read the FIA queue before going into the exam.

170
00:13:53,410 --> 00:13:55,010
So that's the end of this lecture.

171
00:13:55,240 --> 00:13:59,500
I hope you've enjoyed it and if you've got time please join us in the next lecture.

172
00:13:59,500 --> 00:14:00,060
Thank you.
